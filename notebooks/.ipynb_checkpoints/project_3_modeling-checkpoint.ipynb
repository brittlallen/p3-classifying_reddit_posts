{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# importing the packages i'll likely use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_posts_df = pd.read_csv('../data/cleansed_data.csv').drop(columns='Unnamed: 0')\n",
    "\n",
    "# reading in my data and dropping the unnamed axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From maternity photos to bathroom selfies, sho...</td>\n",
       "      <td>Daily Bump Picture Thread - December 19, 2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grab Mr. DeMille and show us dem close ups! Re...</td>\n",
       "      <td>Weekly Ultrasound and Announcement Thread - De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thisismissingtext</td>\n",
       "      <td>Every day since my bump appeared</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thisismissingtext</td>\n",
       "      <td>Was requested to share this here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m in the waiting room right now for my ultra...</td>\n",
       "      <td>Finding out the gender of my baby!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  From maternity photos to bathroom selfies, sho...   \n",
       "1  Grab Mr. DeMille and show us dem close ups! Re...   \n",
       "2                                  thisismissingtext   \n",
       "3                                  thisismissingtext   \n",
       "4  I’m in the waiting room right now for my ultra...   \n",
       "\n",
       "                                               title  target  \n",
       "0      Daily Bump Picture Thread - December 19, 2018       1  \n",
       "1  Weekly Ultrasound and Announcement Thread - De...       1  \n",
       "2                   Every day since my bump appeared       1  \n",
       "3                   Was requested to share this here       1  \n",
       "4                 Finding out the gender of my baby!       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_posts_df.head()\n",
    "\n",
    "# checking the head of my dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['text', 'title']\n",
    "X = reddit_posts_df[features]\n",
    "y = reddit_posts_df.target\n",
    "\n",
    "# creating a features list and setting my X equal to it\n",
    "# setting my Y equal to my target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.50,\n",
    "                                                    stratify=y)\n",
    "\n",
    "# instantiating my train test split\n",
    "# setting a random state for reproducibility and stratifying my Y because the class is slightly unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_text = CountVectorizer(stop_words='english', strip_accents = 'ascii')\n",
    "#cv_title = CountVectorizer(stop_words='english', strip_accents = 'ascii')\n",
    "\n",
    "# instantiating a CountVectorizer on both my text and title features, removing english stop words and stripping ascii accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_text = CountVectorizer(stop_words='english', strip_accents = 'ascii', min_df=.20, max_df= .95)\n",
    "#cv_title = CountVectorizer(stop_words='english', strip_accents = 'ascii')\n",
    "\n",
    "# instantiating a CountVectorizer on both my text and title features, removing english stop words and stripping ascii accents\n",
    "# the assumption was that post texts would have more noise so i set min_df and max_df to help clean it up some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_text = CountVectorizer(stop_words='english', strip_accents = 'ascii', min_df=.20, max_df= .95)\n",
    "#cv_title = CountVectorizer(stop_words='english', strip_accents = 'ascii', ngram_range=(1, 2))\n",
    "\n",
    "# instantiating a CountVectorizer on both my text and title features, removing english stop words and stripping ascii accents\n",
    "# the assumption was that post texts would have more noise so i set min_df and max_df to help clean it up some\n",
    "# the 2nd assumption was that titles were likely more informative & setting an n-gram range of (1, 2) would provide more helpful context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_text = CountVectorizer(stop_words='english', strip_accents = 'ascii', ngram_range=(1, 6), min_df=.03)\n",
    "cv_title = CountVectorizer(stop_words='english', strip_accents = 'ascii', ngram_range=(1, 3), min_df=.01)\n",
    "\n",
    "# instantiating a CountVectorizer on both my text and title features, removing english stop words and stripping ascii accents\n",
    "# the assumption was that post texts with an ngram range of (1, 6) would both clean up noise and be more helpful than those with a range of (1, 1)\n",
    "# i still set a min_df to help clean up noise though it was gentler than previous ones\n",
    "# the assumption was that titles are likely more informative so setting an n-gram range of (1, 4) might provide even more helpful context\n",
    "# i also thought setting a gentle min_df would help clean up any further noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = cv_text.fit_transform(X_train.text)\n",
    "X_train_title = cv_title.fit_transform(X_train.title)\n",
    "\n",
    "X_test_text = cv_text.transform(X_test.text)\n",
    "X_test_title = cv_title.transform(X_test.title)\n",
    "\n",
    "# this will give me two diff matrices one for text and one for title, i'll need to re-combine them before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 336)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text_df = pd.DataFrame(X_train_text.todense(), columns=[x+'_text' for x in cv_text.get_feature_names()])\n",
    "X_train_text_df.shape\n",
    "\n",
    "# creating a dataframe with my train post text and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 57)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title_df = pd.DataFrame(X_train_title.todense(), columns=[y+'_title' for y in cv_title.get_feature_names()])\n",
    "X_train_title_df.shape\n",
    "\n",
    "# creating a dataframe with my train post titles and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 336)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_text_df = pd.DataFrame(X_test_text.todense(), columns=[x+'_text' for x in cv_text.get_feature_names()])\n",
    "X_test_text_df.shape\n",
    "\n",
    "# creating a dataframe with my test post text and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 57)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_title_df = pd.DataFrame(X_test_title.todense(), columns=[y+'_title' for y in cv_title.get_feature_names()])\n",
    "X_test_title_df.shape\n",
    "\n",
    "# creating a dataframe with my test post titles and checking the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecced_train_reddit_posts = pd.concat([X_train_text_df, X_train_title_df], axis=1)\n",
    "vecced_test_reddit_posts = pd.concat([X_test_text_df, X_test_title_df], axis=1)\n",
    "\n",
    "# concatenating my train text and titles back together as well as my test text and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 393)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_train_reddit_posts.shape\n",
    "\n",
    "# checking the shape of my newly concatenated train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 393)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_test_reddit_posts.shape\n",
    "\n",
    "# checking the shape of my newly concatenated train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_train_reddit_posts.isnull().sum().sum()\n",
    "\n",
    "# double-checking to make sure there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecced_test_reddit_posts.isnull().sum().sum()\n",
    "\n",
    "# double-checking to make sure there are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_lr_models(model):\n",
    "    \n",
    "    if model == 'lr_1':\n",
    "        \n",
    "        lr_1_params = {\n",
    "            'penalty': ['l1'],\n",
    "            'C': [1, 1.5, 2, 2.5],\n",
    "            'class_weight': ['balanced'],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [42],\n",
    "            'solver': ['liblinear']}\n",
    "        \n",
    "        M = GridSearchCV(LogisticRegression(),\n",
    "                        lr_1_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'lr_2':\n",
    "        \n",
    "        lr_2_params = {\n",
    "            'penalty': ['l2'],\n",
    "            'C': [1, 1.5, 2, 2.5],\n",
    "            'class_weight': ['balanced'],\n",
    "            'warm_start': [True, False],            \n",
    "            'random_state': [42],\n",
    "            'solver': ['lbfgs', 'liblinear']}\n",
    "        \n",
    "        M = GridSearchCV(LogisticRegression(),\n",
    "                        lr_2_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    M.fit(vecced_train_reddit_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {M.score(vecced_train_reddit_posts.values, y_train)}')\n",
    "    print(f'Test score = {M.score(vecced_test_reddit_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = M.predict(vecced_test_reddit_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(f'Best params = {M.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.9658965896589659\n",
      "Test score = 0.9372937293729373\n",
      "--------\n",
      "[[368  50]\n",
      " [  7 484]]\n",
      "Best params = {'C': 1, 'class_weight': 'balanced', 'penalty': 'l1', 'random_state': 42, 'solver': 'liblinear', 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_lr_models('lr_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Train score = 0.9823982398239824\n",
      "Test score = 0.9306930693069307\n",
      "--------\n",
      "[[370  48]\n",
      " [ 15 476]]\n",
      "Best params = {'C': 1.5, 'class_weight': 'balanced', 'penalty': 'l2', 'random_state': 42, 'solver': 'liblinear', 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_lr_models('lr_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the 3 cells above\n",
    "\n",
    "I defined a function to: \n",
    "- GridSearch the best hyperparameters for two Logistic Regresssion Models\n",
    "- Fit the models\n",
    "- Print the train and test scores for the models\n",
    "- Make predictions and print them in a confusion matrix\n",
    "- Print out the best parameters\n",
    "\n",
    "So in one cell I call <i>run_the_lr_models('lr_1')</i>  which runs the first logistic regression model and in the next cell I call <i>run_the_lr_models('lr_2')</i>  which runs the second model. The difference between the two models are the penalty. Some hyperparemeters only work with an L1 penalty while some only work with the L2 so I separated them based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_dt_models(model):\n",
    "    \n",
    "    if model == 'dt_1':\n",
    "        \n",
    "        dt_params = {\n",
    "            'criterion': ['gini'],\n",
    "            'max_depth': [4, 24, 54],\n",
    "            'min_samples_split': [5, 7, 11, 14],\n",
    "            'max_features': [None, 'log2', 'auto', .40, .50, .70],\n",
    "            'random_state': [42]}\n",
    "        \n",
    "        M = GridSearchCV(DecisionTreeClassifier(),\n",
    "                        dt_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "   \n",
    "    elif model == 'dt_2':\n",
    "        \n",
    "        dt_none_params = {\n",
    "            'criterion': ['entropy'],\n",
    "            'max_depth': [4, 24, 54],\n",
    "            'min_samples_split': [5, 7, 11, 14],\n",
    "            'max_features': [None, 'log2', 'auto', .40, .50, .70],\n",
    "            'random_state': [42]}\n",
    "        \n",
    "        M = GridSearchCV(DecisionTreeClassifier(),\n",
    "                        dt_none_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    M.fit(vecced_train_reddit_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {M.score(vecced_train_reddit_posts.values, y_train)}')\n",
    "    print(f'Test score = {M.score(vecced_test_reddit_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = M.predict(vecced_test_reddit_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(f'Best params = {M.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    3.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.9746974697469747\n",
      "Test score = 0.8921892189218922\n",
      "--------\n",
      "[[356  62]\n",
      " [ 36 455]]\n",
      "Best params = {'criterion': 'gini', 'max_depth': 54, 'max_features': None, 'min_samples_split': 5, 'random_state': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_dt_models('dt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:    3.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.9658965896589659\n",
      "Test score = 0.8976897689768977\n",
      "--------\n",
      "[[353  65]\n",
      " [ 28 463]]\n",
      "Best params = {'criterion': 'entropy', 'max_depth': 54, 'max_features': 0.4, 'min_samples_split': 11, 'random_state': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "run_the_dt_models('dt_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the 3 cells above\n",
    "\n",
    "I defined a function to: \n",
    "- GridSearch the best hyperparameters for two Decision Tree Models\n",
    "- Fit the models\n",
    "- Print the train and test scores for the models\n",
    "- Make predictions and print them in a confusion matrix\n",
    "- Print out the best parameters\n",
    "\n",
    "So in one cell I call <i> run_the_dt_models('dt_1') </i>  which runs the first decision tree model and in the next cell I call <i>run_the_dt_models('dt_2')</i>  which runs the second model. The difference between the two models are the criterion, I wanted one model that focused on gini and the other on entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_rf_models(model):\n",
    "    \n",
    "    if model == 'rf_1':\n",
    "        \n",
    "        rf_params = {\n",
    "            'n_estimators': [15, 24, 30],\n",
    "            'criterion': ['gini'],\n",
    "            'max_depth': [None, 5, 13, 21],\n",
    "            'bootstrap': [True, False],\n",
    "            'min_samples_split': [5, 7, 15, 25],\n",
    "            'max_features': [None, 'log2', 'auto', .10, .25, .50],\n",
    "            'warm_start': [True],\n",
    "            'random_state': [42]}\n",
    "        \n",
    "        M = GridSearchCV(RandomForestClassifier(),\n",
    "                        rf_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'rf_2':\n",
    "        \n",
    "        rf_none_params = {\n",
    "            'n_estimators': [15, 24, 30],\n",
    "            'criterion': ['entropy'],\n",
    "            'max_depth': [None, 5, 13, 21],\n",
    "            'bootstrap': [True, False],\n",
    "            'min_samples_split': [5, 7, 15, 25],\n",
    "            'max_features': [None, 'log2', 'auto',  .10, .25, .50],\n",
    "            'warm_start': [True],\n",
    "            'random_state': [42]}\n",
    "        \n",
    "        M = GridSearchCV(RandomForestClassifier(),\n",
    "                        rf_none_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    M.fit(vecced_train_reddit_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {M.score(vecced_train_reddit_posts.values, y_train)}')\n",
    "    print(f'Test score = {M.score(vecced_test_reddit_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = M.predict(vecced_test_reddit_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(f'Best params = {M.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 451 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1126 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2126 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2776 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.9812981298129813\n",
      "Test score = 0.935093509350935\n",
      "--------\n",
      "[[368  50]\n",
      " [  9 482]]\n",
      "Best params = {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_split': 25, 'n_estimators': 24, 'random_state': 42, 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "run_the_rf_models('rf_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 451 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 947 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1397 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1996 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2738 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.9834983498349835\n",
      "Test score = 0.933993399339934\n",
      "--------\n",
      "[[374  44]\n",
      " [ 16 475]]\n",
      "Best params = {'bootstrap': False, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'min_samples_split': 5, 'n_estimators': 24, 'random_state': 42, 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "run_the_rf_models('rf_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the 3 cells above\n",
    "\n",
    "I defined a function to: \n",
    "- GridSearch the best hyperparameters for two Random Forest Models\n",
    "- Fit the models\n",
    "- Print the train and test scores for the models\n",
    "- Make predictions and print them in a confusion matrix\n",
    "- Print out the best parameters\n",
    "\n",
    "So in one cell I call <i>run_the_rf_models('rf_1')</i>  which runs the first random forest model and in the next cell I call <i>run_the_rf_models('rf_2')</i>  which runs the second model. The difference between the two models are the criterion, I wanted one model that focused on gini and the other on entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTINOMIALNB MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_mn_models(model):\n",
    "    \n",
    "    if model == 'mn_1':\n",
    "        \n",
    "        mn_params = {\n",
    "            'fit_prior': [True],\n",
    "            'alpha': [0, 0.5, 1]}\n",
    "        \n",
    "        M = GridSearchCV(MultinomialNB(),\n",
    "                        mn_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    elif model == 'mn_2':\n",
    "        \n",
    "         mn_params = {\n",
    "            'fit_prior': [False],\n",
    "            'alpha': [0, 0.5, 1]}\n",
    "        \n",
    "         M = GridSearchCV(MultinomialNB(),\n",
    "                        mn_params,\n",
    "                        cv = 5,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "    else:\n",
    "        print('There is an error.')\n",
    "        \n",
    "    M.fit(vecced_train_reddit_posts.values, y_train)\n",
    "     \n",
    "    print(f'Train score = {M.score(vecced_train_reddit_posts.values, y_train)}')\n",
    "    print(f'Test score = {M.score(vecced_test_reddit_posts.values, y_test)}')\n",
    "    \n",
    "    predictions = M.predict(vecced_test_reddit_posts.values)\n",
    "    print('--------')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(f'Best params = {M.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score = 0.9262926292629263\n",
      "Test score = 0.9240924092409241\n",
      "--------\n",
      "[[399  19]\n",
      " [ 50 441]]\n",
      "Best params = {'alpha': 0, 'fit_prior': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.4s finished\n",
      "C:\\Users\\anybritt\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "run_the_mn_models('mn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Train score = 0.9240924092409241\n",
      "Test score = 0.9218921892189219\n",
      "--------\n",
      "[[401  17]\n",
      " [ 54 437]]\n",
      "Best params = {'alpha': 0, 'fit_prior': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.4s finished\n",
      "C:\\Users\\anybritt\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "run_the_mn_models('mn_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the 3 cells above\n",
    "\n",
    "I defined a function to: \n",
    "- GridSearch the best hyperparameters for two Multinomial NB Models\n",
    "- Fit the models\n",
    "- Print the train and test scores for the models\n",
    "- Make predictions and print them in a confusion matrix\n",
    "- Print out the best parameters\n",
    "\n",
    "So in one cell I call <i>run_the_mn_models('mn_1')</i>  which runs the first multinomial nb model and in the next cell I call <i>run_the_mn_models('mn_2')</i>  which runs the second model. The difference between the two models are fit_prior, with one model having it set equal to True and the other to False."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
